{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21bf41b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Ibrahim\n",
      "[nltk_data]     Alassaf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Ibrahim\n",
      "[nltk_data]     Alassaf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e2d4860",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import os\n",
    "import string\n",
    "import copy\n",
    "import pickle \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba153ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ibrahim Alassaf\\Desktop\\lab7\\informationRetrieval_lab7\\informationRetrieval_lab7\\informationRetrieval_lab7\\20_newsgroups/alt.atheism/\n"
     ]
    }
   ],
   "source": [
    "title = \"alt.atheism\"\n",
    "os.chdir(\"C:/Users/Ibrahim Alassaf/Desktop\\lab7/informationRetrieval_lab7/informationRetrieval_lab7/informationRetrieval_lab7/20_newsgroups/\")\n",
    "paths = []\n",
    "for (dirpath, dirnames, filenames) in os.walk(str(os.getcwd())+'/'+title+'/'):\n",
    "   for i in filenames:\n",
    "      paths.append(str(dirpath)+str(\"\\\\\")+i) \n",
    "\n",
    "print(dirpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b83e9857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ibrahim Alassaf\\Desktop\\lab7\\informationRetrieval_lab7\\informationRetrieval_lab7\\informationRetrieval_lab7\\20_newsgroups/alt.atheism/\\49960\n"
     ]
    }
   ],
   "source": [
    "print(paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18d4bf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(data):\n",
    " stop_words = stopwords.words('english')\n",
    " words = word_tokenize(str(data))\n",
    " new_text = \" \"\n",
    " for w in words:\n",
    "   if w not in stop_words:\n",
    "       new_text = new_text + \" \" + w\n",
    " return np.char.strip(new_text) \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9986d09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(data):\n",
    "    symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
    "    for i in range(len(symbols)):\n",
    "        data = np.char.replace(data, symbols[i], ' ')\n",
    "        data = np.char.replace(data, \"  \", \" \")\n",
    "    data = np.char.replace(data, ',', '')\n",
    "    return data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63d4d265",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_lower_case(data):\n",
    " return np.char.lower(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7015ba61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stemming(data):\n",
    " stemmer= PorterStemmer()\n",
    "\n",
    " tokens = word_tokenize(str(data))\n",
    " new_text = \"\"\n",
    " for w in tokens:\n",
    "    new_text = new_text + \" \" + stemmer.stem(w)\n",
    " return np.char.strip(new_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b8981d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_numbers(data):\n",
    " data = np.char.replace(data, \"0\", \" zero \")\n",
    " data = np.char.replace(data, \"1\", \" one \")\n",
    " data = np.char.replace(data, \"2\", \" two \")\n",
    " data = np.char.replace(data, \"3\", \" three \")\n",
    " data = np.char.replace(data, \"4\", \" four \")\n",
    " data = np.char.replace(data, \"5\", \" five \")\n",
    " data = np.char.replace(data, \"6\", \" six \")\n",
    " data = np.char.replace(data, \"7\", \" seven \")\n",
    " data = np.char.replace(data, \"8\", \" eight \")\n",
    " data = np.char.replace(data, \"9\", \" nine \")\n",
    " return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5573a52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_header(data):\n",
    "    try:\n",
    "        ind = data.index('\\n\\n')\n",
    "        data = data[ind:]\n",
    "    except:\n",
    "         print(\"No Header\")\n",
    "    return data \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7c82d695",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_apostrophe(data):\n",
    "    return np.char.replace(data, \"'\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d069abb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_single_characters(data):\n",
    "    words = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in words:\n",
    "         if len(w) > 1:\n",
    "            new_text = new_text + \" \" + w\n",
    "    return np.char.strip(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b66058d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data,query): \n",
    "    data = remove_header(data)\n",
    "    data = convert_lower_case(data)\n",
    "    data = remove_punctuation(data)\n",
    "    data = remove_stop_words(data)\n",
    "    data = remove_apostrophe(data)\n",
    "    data = remove_single_characters(data)\n",
    "    data = stemming(data)\n",
    "    return data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "92f63aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ibrahim Alassaf\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3364: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead.  To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  if (await self.run_code(code, result,  async_=asy)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "doc = 0\n",
    "postings = pd.DataFrame()\n",
    "for path in paths:\n",
    "    file = open(path, 'r', encoding='cp1250')\n",
    "    text = file.read().strip()\n",
    "    file.close()\n",
    "    preprocessed_text = preprocess(text, False)\n",
    "\n",
    "    if doc%100 == 0:\n",
    "        print(doc)\n",
    "\n",
    "    tokens = word_tokenize(str(preprocessed_text))\n",
    "    for token in tokens:\n",
    "        if token in postings:\n",
    "            p = postings[token][0]\n",
    "            p.add(doc)\n",
    "            postings[token][0] = p \n",
    "        else:\n",
    "           postings.insert(value=[{doc}], loc=0,column=token)\n",
    "doc += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07ca14c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zombi</th>\n",
       "      <th>husband</th>\n",
       "      <th>thinker</th>\n",
       "      <th>mix</th>\n",
       "      <th>drown</th>\n",
       "      <th>chimp</th>\n",
       "      <th>fought</th>\n",
       "      <th>judgment</th>\n",
       "      <th>gentil</th>\n",
       "      <th>altruist</th>\n",
       "      <th>...</th>\n",
       "      <th>1992</th>\n",
       "      <th>decemb</th>\n",
       "      <th>11</th>\n",
       "      <th>modifi</th>\n",
       "      <th>last</th>\n",
       "      <th>alt</th>\n",
       "      <th>resourc</th>\n",
       "      <th>atheism</th>\n",
       "      <th>name</th>\n",
       "      <th>archiv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{0}</td>\n",
       "      <td>{0}</td>\n",
       "      <td>{0}</td>\n",
       "      <td>{0}</td>\n",
       "      <td>{0}</td>\n",
       "      <td>{0}</td>\n",
       "      <td>{0}</td>\n",
       "      <td>{0}</td>\n",
       "      <td>{0}</td>\n",
       "      <td>{0}</td>\n",
       "      <td>...</td>\n",
       "      <td>{0}</td>\n",
       "      <td>{0}</td>\n",
       "      <td>{0}</td>\n",
       "      <td>{0}</td>\n",
       "      <td>{0}</td>\n",
       "      <td>{0}</td>\n",
       "      <td>{0}</td>\n",
       "      <td>{0}</td>\n",
       "      <td>{0}</td>\n",
       "      <td>{0}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 5567 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  zombi husband thinker  mix drown chimp fought judgment gentil altruist  ...  \\\n",
       "0   {0}     {0}     {0}  {0}   {0}   {0}    {0}      {0}    {0}      {0}  ...   \n",
       "\n",
       "  1992 decemb   11 modifi last  alt resourc atheism name archiv  \n",
       "0  {0}    {0}  {0}    {0}  {0}  {0}     {0}     {0}  {0}    {0}  \n",
       "\n",
       "[1 rows x 5567 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a6227e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "postings[\"exam\"][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "268289f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99,\n",
       " 100,\n",
       " 101,\n",
       " 102,\n",
       " 103,\n",
       " 104,\n",
       " 105,\n",
       " 106,\n",
       " 107,\n",
       " 108,\n",
       " 109,\n",
       " 110,\n",
       " 111,\n",
       " 112,\n",
       " 113,\n",
       " 114,\n",
       " 115,\n",
       " 116,\n",
       " 117,\n",
       " 118,\n",
       " 119,\n",
       " 120,\n",
       " 121,\n",
       " 122,\n",
       " 123,\n",
       " 124,\n",
       " 125,\n",
       " 126,\n",
       " 127,\n",
       " 128,\n",
       " 129,\n",
       " 130,\n",
       " 131,\n",
       " 132,\n",
       " 133,\n",
       " 134,\n",
       " 135,\n",
       " 136,\n",
       " 137,\n",
       " 138,\n",
       " 139,\n",
       " 140,\n",
       " 141,\n",
       " 142,\n",
       " 143,\n",
       " 144,\n",
       " 145,\n",
       " 146,\n",
       " 147,\n",
       " 148,\n",
       " 149,\n",
       " 150,\n",
       " 151,\n",
       " 152,\n",
       " 153,\n",
       " 154,\n",
       " 155,\n",
       " 156,\n",
       " 157,\n",
       " 158,\n",
       " 159,\n",
       " 160,\n",
       " 161,\n",
       " 162,\n",
       " 163,\n",
       " 164,\n",
       " 165,\n",
       " 166,\n",
       " 167,\n",
       " 168,\n",
       " 169,\n",
       " 170,\n",
       " 171,\n",
       " 172,\n",
       " 173,\n",
       " 174,\n",
       " 175,\n",
       " 176,\n",
       " 177,\n",
       " 178,\n",
       " 179,\n",
       " 180,\n",
       " 181,\n",
       " 182,\n",
       " 183,\n",
       " 184,\n",
       " 185,\n",
       " 186,\n",
       " 187,\n",
       " 188,\n",
       " 189,\n",
       " 190,\n",
       " 191,\n",
       " 192,\n",
       " 193,\n",
       " 194,\n",
       " 195,\n",
       " 196,\n",
       " 197,\n",
       " 198,\n",
       " 199,\n",
       " 200,\n",
       " 201,\n",
       " 202,\n",
       " 203,\n",
       " 204,\n",
       " 205,\n",
       " 206,\n",
       " 207,\n",
       " 208,\n",
       " 209,\n",
       " 210,\n",
       " 211,\n",
       " 212,\n",
       " 213,\n",
       " 214,\n",
       " 215,\n",
       " 216,\n",
       " 217}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_not(word):\n",
    "    a = postings[word][0]\n",
    "    b = set(range(len(paths)))\n",
    "    return b.difference(a)\n",
    "\n",
    "get_not(\"one\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c026a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_command_tokens(query):\n",
    "    query = query.lower()\n",
    "    tokens = word_tokenize(query)\n",
    "    \n",
    "    commands = []\n",
    "    query_words = []\n",
    "    \n",
    "    for t in tokens:\n",
    "        if t not in ['and', 'or', 'not']:\n",
    "            preprocessed_word = preprocess([t], True)\n",
    "            print(str(preprocessed_word))\n",
    "            query_words.append(str(preprocessed_word))\n",
    "        else:\n",
    "            commands.append(t)\n",
    "            \n",
    "    return commands, query_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb489044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_not_tuple(query_words, commands):\n",
    "\n",
    "    tup = []\n",
    "\n",
    "    while 'not' in commands:\n",
    "\n",
    "        i = commands.index('not')\n",
    "\n",
    "        word = query_words[i]\n",
    "\n",
    "        word_postings = get_not(word)\n",
    "\n",
    "        tup.append(word_postings)\n",
    "\n",
    "        commands.pop(i)\n",
    "\n",
    "        query_words[i] = i\n",
    "\n",
    "        print('\\nAfter Not Processing:', commands, query_words)\n",
    "\n",
    "    return tup\n",
    "\n",
    "\n",
    "def binary_operations(query_words, commands, tup):\n",
    "\n",
    "    a = postings[query_words[0]][0]\n",
    "\n",
    "    query_words.pop(0)\n",
    "\n",
    "    for i in range(len(commands)):\n",
    "\n",
    "        if type(query_words[i]) == int:\n",
    "\n",
    "            b = tup.pop(0)\n",
    "        else:\n",
    "\n",
    "            b = postings[query_words[i]][0]\n",
    "\n",
    "        if commands[i] == 'and':\n",
    "\n",
    "            a = a.intersection(b)\n",
    "        elif commands[i] == 'or':\n",
    "\n",
    "            a = a.union(b)\n",
    "        else:\n",
    "\n",
    "            print('Invalid Command')\n",
    "\n",
    "    return a\n",
    "\n",
    "\n",
    "def execute_query(query):\n",
    "\n",
    "    (commands, query_words) = generate_command_tokens(query)\n",
    "\n",
    "    tup = gen_not_tuple(query_words, commands)\n",
    "\n",
    "    print('\\nCommands:', commands)\n",
    "\n",
    "    print('\\nQuery Words:', query_words)\n",
    "\n",
    "    print('\\nTup:', len(tup))\n",
    "\n",
    "    final_set = binary_operations(query_words, commands, tup)\n",
    "\n",
    "    print('\\nFinal Set:', final_set)\n",
    "\n",
    "    return final_set\n",
    "\n",
    "\n",
    "def print_file(file):\n",
    "\n",
    "    out_file = open(paths[file], 'r', encoding='cp1250')\n",
    "\n",
    "    out_text = out_file.read()\n",
    "\n",
    "    print(out_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1a310f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \" exam and not resource\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1cc9e28e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Header\n",
      "exam\n",
      "No Header\n",
      "resourc\n",
      "\n",
      "After Not Processing: ['and'] ['exam', 1]\n",
      "\n",
      "Commands: ['and']\n",
      "\n",
      "Query Words: ['exam', 1]\n",
      "\n",
      "Tup: 1\n",
      "\n",
      "Final Set: set()\n"
     ]
    }
   ],
   "source": [
    "lists = execute_query(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "485c3ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ibrahim Alassaf\\Desktop\\lab7\\informationRetrieval_lab7\\informationRetrieval_lab7\\informationRetrieval_lab7\\20_newsgroups/comp.graphics/\n"
     ]
    }
   ],
   "source": [
    "title = \"comp.graphics\"\n",
    "os.chdir(\"C:/Users/Ibrahim Alassaf/Desktop/lab7/informationRetrieval_lab7/informationRetrieval_lab7/informationRetrieval_lab7/20_newsgroups\")\n",
    "\n",
    "paths = []\n",
    "for(dirpath, dirnames, filenames) in os.walk(str(os.getcwd())+'/'+title+'/'): \n",
    "    for i in filenames:\n",
    "        paths.append(str(dirpath)+str(\"\\\\\")+i)\n",
    "\n",
    "print(dirpath)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f4fcf5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "postings = pd.DataFrame()\n",
    "frequency = pd.DataFrame() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9209fe8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "No Header\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "No Header\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "doc = 0\n",
    "for path in paths:\n",
    "    file = open(path, 'r', encoding='cp1250')\n",
    "    text = file.read().strip()\n",
    "    file.close()\n",
    "    preprocessed_text = preprocess(text, False)\n",
    "    if doc%100 == 0:\n",
    "        print(doc)\n",
    "        tokens = word_tokenize(str(preprocessed_text)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7749d2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = 0\n",
    "for token in tokens:\n",
    "    if token in postings:\n",
    "        p = postings[token][0]\n",
    "        k = [a[0] for a in p]\n",
    "        if doc in k:\n",
    "            for a in p:\n",
    "                if a[0] == doc:\n",
    "                    a[1].add(pos)\n",
    "                else:\n",
    "                    p.append([doc,{pos}])\n",
    "                    frequency[token][0] += 1\n",
    "        else:\n",
    "            postings.insert(value=[[[doc, {pos}]]],\n",
    "            loc=0, column=token)\n",
    "            frequency.insert(value=[1], loc=0, column=token)\n",
    "            pos += 1\n",
    "            doc += 1 \n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3c96f24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "postings.to_pickle(title + \"_positional_postings\")\n",
    "frequency.to_pickle(title + \"_positional_frequency\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "797b0799",
   "metadata": {},
   "outputs": [],
   "source": [
    "postings = pd.read_pickle(title + \"_positional_postings\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2bc17362",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency = pd.read_pickle(title + \"_positional_frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d0534fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#posting index\n",
    "def get_word_postings(word):\n",
    "    preprocessed_word = str(preprocess(word, True))\n",
    "    print(preprocessed_word)\n",
    "    print(\"Frequency:\",frequency[preprocessed_word][0])\n",
    "    print(\"Postings List:\",postings[preprocessed_word][0])\n",
    "\n",
    "def get_positions(posting_values, doc):\n",
    "    for posting_value in posting_values:\n",
    "        if posting_value[0] == doc:\n",
    "            return posting_value[0]\n",
    "    return {}\n",
    "\n",
    "def gen_init_set_matchings(word):\n",
    "    init = []\n",
    "    word_postings = postings[word][0]\n",
    "    for word_postings in word_postings:\n",
    "        for positions in word_postings[1]:\n",
    "            init.append((word_postings[0],positions))\n",
    "    return init\n",
    "\n",
    "def match_positional_index(init,b):\n",
    "    matched_docs = []\n",
    "    for p in init:\n",
    "        doc = p[0]\n",
    "        pos = p[1]\n",
    "        count = 0\n",
    "        for k in b:\n",
    "            pos = pos+1\n",
    "            k_pos = postings[k][0]\n",
    "            docs_list = [z[0] for z in k_pos]\n",
    "            if doc in docs_list:\n",
    "                doc_positions = get_positions(k_pos,doc)\n",
    "                if pos in doc_positions:\n",
    "                    count+= 1\n",
    "                else:\n",
    "                    count+= 1\n",
    "                    break\n",
    "                if count == len(b):\n",
    "                    matched_docs.append(p[0])\n",
    "    return set(matched_docs)\n",
    "\n",
    "\n",
    "\n",
    "def run_query(query):\n",
    "    processed_query = preprocess(query, True)\n",
    "    print(processed_query)\n",
    "    query_tokens = word_tokenize(str(processed_query))\n",
    "    print(query_tokens)\n",
    "\n",
    "    if len(query_tokens)==1:\n",
    "        print(\"Total Document Mathces\",[a[0]for a in postings[query][0]])\n",
    "        return [a[0]for a in postings[query][0]]\n",
    "\n",
    "\n",
    "    init_word = query_tokens[0]\n",
    "    init_matches = gen_init_set_matchings(init_word)\n",
    "\n",
    "    query_tokens.pop(0)\n",
    "    total_matched_docs = match_positional_index(init_matches,query_tokens)\n",
    "    print(\"Total Document Matches:\", total_matched_docs)\n",
    "    return total_matched_docs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c683eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_word_postings(\"welcome\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f978938",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"call\"\n",
    "\n",
    "list = run_query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f607b45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
