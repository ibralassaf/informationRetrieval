{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Lab Exercise 1: Introduction to Lab Environment & Gensim"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1 - Environment Installation "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise 1: Do you remember how to check python version and installed libraries? Write steps below"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "#To check the python version\r\n",
    "from platform import python_version\r\n",
    "\r\n",
    "print(\"pythom version is : \" + python_version())\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "pythom version is : 3.8.8\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "#I imported the numpy and pandas librarys then checked the versions\r\n",
    "import numpy\r\n",
    "import pandas as pd\r\n",
    "print(\"numpy version is : \" + numpy.version.version)\r\n",
    "print(\"pandas version is : \" + pd.__version__)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "numpy version is : 1.20.1\n",
      "pandas version is : 1.2.4\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Installing gensim and check the version"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#To install gensim here or from conda (I used anaconda terminal to install it) \r\n",
    "pip install gensim"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "import gensim as gs\r\n",
    "\r\n",
    "print(\"gensim version is : \" + gs.__version__)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "gensim version is : 4.1.0\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2 - Working with Gensim\r\n",
    "Core Concepts of Gensim\r\n",
    "Following are the core concepts and terms that are needed to understand and use\r\n",
    "Gensim:<br>\r\n",
    "<ol>\r\n",
    "<li> Document: It refers to some text. </li>\r\n",
    "<li> Corpus: It refers to a collection of documents.</li>\r\n",
    "<li> Vector: Mathematical representation of a document is called vector.</li>\r\n",
    "<li> Model: It refers to an algorithm used for transforming vectors from one\r\n",
    "representation to another. </li>\r\n",
    "</ol>"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise 2: Tokenize the following sentence and write down the result you obtain! "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "#This is a variable to store the text\r\n",
    "Sentence= \"Tokenization is the process of breaking down text document apart into those pieces\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "#Imported the gensim library then tokenized the text from Sentence varibale\r\n",
    "import gensim as gs\r\n",
    "tokenizedWord = list(gs.utils.tokenize(Sentence))\r\n",
    "print(tokenizedWord)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Tokenization', 'is', 'the', 'process', 'of', 'breaking', 'down', 'text', 'document', 'apart', 'into', 'those', 'pieces']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "#For helpful commands\r\n",
    "help(gs.utils.tokenize)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Help on function tokenize in module gensim.utils:\n",
      "\n",
      "tokenize(text, lowercase=False, deacc=False, encoding='utf8', errors='strict', to_lower=False, lower=False)\n",
      "    Iteratively yield tokens as unicode strings, optionally removing accent marks and lowercasing it.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    text : str or bytes\n",
      "        Input string.\n",
      "    deacc : bool, optional\n",
      "        Remove accentuation using :func:`~gensim.utils.deaccent`?\n",
      "    encoding : str, optional\n",
      "        Encoding of input string, used as parameter for :func:`~gensim.utils.to_unicode`.\n",
      "    errors : str, optional\n",
      "        Error handling behaviour, used as parameter for :func:`~gensim.utils.to_unicode`.\n",
      "    lowercase : bool, optional\n",
      "        Lowercase the input string?\n",
      "    to_lower : bool, optional\n",
      "        Same as `lowercase`. Convenience alias.\n",
      "    lower : bool, optional\n",
      "        Same as `lowercase`. Convenience alias.\n",
      "    \n",
      "    Yields\n",
      "    ------\n",
      "    str\n",
      "        Contiguous sequences of alphabetic characters (no digits!), using :func:`~gensim.utils.simple_tokenize`\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    .. sourcecode:: pycon\n",
      "    \n",
      "        >>> from gensim.utils import tokenize\n",
      "        >>> list(tokenize('Nic nemůže letět rychlostí vyšší, než 300 tisíc kilometrů za sekundu!', deacc=True))\n",
      "        [u'Nic', u'nemuze', u'letet', u'rychlosti', u'vyssi', u'nez', u'tisic', u'kilometru', u'za', u'sekundu']\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Exercise 3: Count frequency of each word.\r\n",
    "\r\n",
    "Sentence= “In computer science, artificial<br>\r\n",
    "intelligence (AI), sometimes called machine<br>\r\n",
    "intelligence, is in telligence demonstrated by<br>\r\n",
    "machines, in contrast to the natural intelligence<br>\r\n",
    "displayed by humans and animals. Computer science<br>\r\n",
    "defines AI research as the study of intelligent<br>\r\n",
    "agents: any device that perceives its environment and<br>\r\n",
    "takes actions that maximize its chance of successfully<br>\r\n",
    "achieving its goals.” \r\n",
    "\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "#Imported the librarys and used the Bag of words way to map the words with ID\r\n",
    "from gensim import corpora \r\n",
    "from pprint import pprint\r\n",
    "text = [\"\"\"In computer science, artificial\r\n",
    "intelligence (AI), sometimes called machine\r\n",
    "intelligence, is intelligence demonstrated by\r\n",
    "machines, in contrast to the natural intelligence\r\n",
    "displayed by humans and animals. Computer science\r\n",
    "defines AI research as the study of intelligent\r\n",
    "agents: any device that perceives its environment and\r\n",
    "takes actions that maximize its chance of successfully\r\n",
    "achieving its goals.\"\"\"]\r\n",
    "tokens = [[token for token in sentence.split()] for sentence in text]\r\n",
    "gensim_dictionary = corpora.Dictionary()\r\n",
    "gensim_corpus = [gensim_dictionary.doc2bow(token,allow_update=True) for token in tokens]\r\n",
    "print(gensim_corpus)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 2), (8, 1), (9, 1), (10, 1), (11, 1), (12, 2), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 3), (26, 1), (27, 1), (28, 1), (29, 3), (30, 1), (31, 1), (32, 1), (33, 1), (34, 2), (35, 1), (36, 1), (37, 1), (38, 1), (39, 1), (40, 1), (41, 1), (42, 1), (43, 2), (44, 2), (45, 1)]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## What do you see?\r\n",
    "\r\n",
    "I see tuples (0,1 ) & (1,1) etc that means word with ID 0 appeared 1 time in the text and so on "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "#this code adding frequency count to make thing clear\r\n",
    "word_frequencies = [[(gensim_dictionary[id],\r\n",
    "frequence) for id, frequence in couple] for couple in\r\n",
    "gensim_corpus]\r\n",
    "print(word_frequencies) "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[('(AI),', 1), ('AI', 1), ('Computer', 1), ('In', 1), ('achieving', 1), ('actions', 1), ('agents:', 1), ('and', 2), ('animals.', 1), ('any', 1), ('artificial', 1), ('as', 1), ('by', 2), ('called', 1), ('chance', 1), ('computer', 1), ('contrast', 1), ('defines', 1), ('demonstrated', 1), ('device', 1), ('displayed', 1), ('environment', 1), ('goals.', 1), ('humans', 1), ('in', 1), ('intelligence', 3), ('intelligence,', 1), ('intelligent', 1), ('is', 1), ('its', 3), ('machine', 1), ('machines,', 1), ('maximize', 1), ('natural', 1), ('of', 2), ('perceives', 1), ('research', 1), ('science', 1), ('science,', 1), ('sometimes', 1), ('study', 1), ('successfully', 1), ('takes', 1), ('that', 2), ('the', 2), ('to', 1)]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Home exercise:\r\n",
    "Create a bag of words corpus by reading a text file.\r\n",
    "Hint: use genism.utils.simple_preprocess \r\n",
    "\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "#Import the mail librarys like usuall then use simple_preprocess like it says in the lab 1 pdf and smart open this makes you open external document so the gensim will read the file line by line using simple_prerocess and os library important for the path\r\n",
    "import gensim\r\n",
    "from gensim import corpora\r\n",
    "from pprint import pprint\r\n",
    "from gensim.utils import simple_preprocess\r\n",
    "from smart_open import smart_open\r\n",
    "import os\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "#Source: simple_preprocess(doc, deacc=False, min_len=2, max_len=15 | Convert a document into a list of tokens, This lowercases, tokenizes, de-accents (optional). – the output are final tokens = unicode strings, that won’t be processed any further.\r\n",
    "\r\n",
    "tokenizedDocument = [\r\n",
    "   simple_preprocess(doc, deacc =True) for doc in open('document.txt', encoding='utf-8')\r\n",
    "]\r\n",
    "#initilize a dictionary from corpora and do doc2bow which iterates through all the words in the text, if the word already exists in the corpus\r\n",
    "dictionary = corpora.Dictionary()\r\n",
    "bagOfwords = [dictionary.doc2bow(doc, allow_update=True) for doc in tokenizedDocument]\r\n",
    "print(bagOfwords)\r\n",
    "\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1)]]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "So in conclusion I learned how to open a document and tokenized it with simple preprocess then initilize it and do doc2bow like we learned in the class and print the bag of words "
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "ca6373b9b166d3015c88dcbfbb8a8050c882ec00cdf00c59953f1835a475a5ba"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}